{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS TASK"
      ],
      "metadata": {
        "id": "fp2ESUxQv25r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare 3 configurations for the activation function. Show and explain your performance result"
      ],
      "metadata": {
        "id": "pyPuRaAbg_Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all libraries\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "iF--Gg5Kp8X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset for the data loader\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "x5g_HRlKkz0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8716b8-3fcf-485f-a8ad-73f82a2b4f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 103032080.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 76233916.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26038649.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13966663.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membandingkan beragam konfigurasi untuk fungsi aktivasi. Dalam percobaan ini, kami akan melatih dan mengevaluasi tiga model berbeda yang menggunakan pendekatan fungsi aktivasi berbeda.\n",
        "\n",
        "\n",
        "*   Model 1: Fungsi aktivasi ReLU. Fungsi aktivasi ini diterapkan pada lapisan pertama (fc1) dan membantu mempercepat konvergensi selama pelatihan.\n",
        "*   Model 2: Fungsi aktivasi Sigmoid. Fungsi ini menyesuaikan keluaran lapisan pertama ke kisaran antara 0 dan 1, terbukti berguna untuk tugas klasifikasi biner.\n",
        "*   Model 3: Fungsi aktivasi Softmax. Fungsi ini diterapkan pada lapisan keluaran (fc2) untuk menghasilkan distribusi probabilitas, khususnya bermanfaat untuk tugas klasifikasi kelas jamak.\n"
      ],
      "metadata": {
        "id": "J-rD0pp9O2dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ReLU activation funcion**"
      ],
      "metadata": {
        "id": "O5rBsDEaOGtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1: ReLU activation function\n",
        "\n",
        "class Model1(nn.Module): # Define a neural network model named Model1 that inherits from nn.Module\n",
        "  def __init__(self): # Constructor method (__init__) initializes the object when an instance is created\n",
        "      super(Model1, self).__init__() # Initialize the Model1 class as a subclass of nn.Module\n",
        "      self.fc1 = nn.Linear(28*28, 128) # Input size: 28*28, Output size: 128\n",
        "      self.fc2 = nn.Linear(128, 10) # Input size: 128, Output size: 10\n",
        "\n",
        "  def forward(self, x): # Forward method defines the forward pass of the neural network\n",
        "      x = x.view(-1, 28*28) # Reshape the input tensor x to have dimensions (-1, 28*28)\n",
        "      x = torch.relu(self.fc1(x)) # Apply the ReLU activation function to the output of fc1\n",
        "      x = self.fc2(x) # Pass the result through fc2 (no activation function here)\n",
        "      return x # Return the final output"
      ],
      "metadata": {
        "id": "aT0TbP5xkJhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid activation function**"
      ],
      "metadata": {
        "id": "4xLnBZfOodtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: Sigmoid activation function\n",
        "\n",
        "class Model2(nn.Module): # Define a neural network model named Model2 that inherits from nn.Module\n",
        "  def __init__(self): # Constructor method (__init__) initializes the object when an instance is created\n",
        "      super(Model2, self).__init__() # Initialize the Model2 class as a subclass of nn.Module\n",
        "      self.fc1 = nn.Linear(28*28, 128) # Input size: 28*28, Output size: 128\n",
        "      self.fc2 = nn.Linear(128, 10) # Input size: 128, Output size: 10\n",
        "\n",
        "  def forward(self, x):  # Forward method defines the forward pass of the neural network\n",
        "      x = x.view(-1, 28*28) # Reshape the input tensor x to have dimensions (-1, 28*28)\n",
        "      x = torch.sigmoid(self.fc1(x)) # Apply the sigmoid activation function to the output of fc1\n",
        "      x = self.fc2(x) # Pass the result through fc2 (no activation function here)\n",
        "      return x # Return the final output"
      ],
      "metadata": {
        "id": "FLSTyF_V2n2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax activation function**"
      ],
      "metadata": {
        "id": "OUMtagayo1a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3: Softmax activation function\n",
        "\n",
        "class Model3(nn.Module): # Define a neural network model named Model3 that inherits from nn.Module\n",
        "  def __init__(self): # Constructor method (__init__) initializes the object when an instance is created\n",
        "      super(Model3, self).__init__() # Initialize the Model2 class as a subclass of nn.Module\n",
        "      self.fc1 = nn.Linear(28*28, 128)  # Input size: 28*28, Output size: 128\n",
        "      self.fc2 = nn.Linear(128, 10) # Input size: 128, Output size: 10\n",
        "\n",
        "  def forward(self, x): # Forward method defines the forward pass of the neural network\n",
        "      x = x.view(-1, 28*28) # Reshape the input tensor x to have dimensions (-1, 28*28)\n",
        "      x = self.fc1(x)  # Pass the input through fc1 without an activation function\n",
        "      x = torch.softmax(x, dim=1) # Apply the softmax activation function to the output of fc1 along dimension 1\n",
        "      x = self.fc2(x) # Pass the result through fc2 without an activation function\n",
        "      return x # Return the final output"
      ],
      "metadata": {
        "id": "osdSNiWB3WO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function menggunakan Cross Entropy loss function"
      ],
      "metadata": {
        "id": "k96nXSxUkMLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_el = nn.CrossEntropyLoss() # Using Cross Entropy loss function\n",
        "learning_rate = 0.01 # Configuring the learning rate\n",
        "momentum = 0.9 # Configuring momentum\n",
        "num_epochs = 5 # Number of epochs or training iterations"
      ],
      "metadata": {
        "id": "bVe19uRg4cf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan konfigurasi ini, model dapat dilatih menggunakan pendekatan pelatihan yang sesuai untuk Cross Entropy Loss Function, dengan kecepatan pembelajaran, momentum, dan jumlah epoch tertentu."
      ],
      "metadata": {
        "id": "JYDaZ3EkaTi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [Model1(), Model2(), Model3()]\n",
        "accuracies = []\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "  print(f\"Model {i+1}\")\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for j, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = cross_el(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if j % 100 == 99:\n",
        "            print(f\"[{epoch+1}, {j+1}] loss: {running_loss/100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          images, labels = data\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f\"Accuracy: {accuracy}%\")\n",
        "  accuracies.append(accuracy)\n",
        "\n",
        "print(\"Accuracies:\", accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqSfckclFBei",
        "outputId": "eb80e810-00f2-4024-b26d-363aecb3dd09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "[1, 100] loss: 1.227\n",
            "[1, 200] loss: 0.660\n",
            "[1, 300] loss: 0.681\n",
            "[1, 400] loss: 0.677\n",
            "[1, 500] loss: 0.583\n",
            "[1, 600] loss: 0.563\n",
            "[1, 700] loss: 0.489\n",
            "[1, 800] loss: 0.436\n",
            "[1, 900] loss: 0.424\n",
            "[1, 1000] loss: 0.505\n",
            "[1, 1100] loss: 0.443\n",
            "[1, 1200] loss: 0.434\n",
            "[1, 1300] loss: 0.459\n",
            "[1, 1400] loss: 0.446\n",
            "[1, 1500] loss: 0.491\n",
            "[1, 1600] loss: 0.402\n",
            "[1, 1700] loss: 0.400\n",
            "[1, 1800] loss: 0.407\n",
            "[1, 1900] loss: 0.400\n",
            "[1, 2000] loss: 0.343\n",
            "[1, 2100] loss: 0.369\n",
            "[1, 2200] loss: 0.480\n",
            "[1, 2300] loss: 0.479\n",
            "[1, 2400] loss: 0.386\n",
            "[1, 2500] loss: 0.325\n",
            "[1, 2600] loss: 0.378\n",
            "[1, 2700] loss: 0.415\n",
            "[1, 2800] loss: 0.377\n",
            "[1, 2900] loss: 0.335\n",
            "[1, 3000] loss: 0.364\n",
            "[1, 3100] loss: 0.349\n",
            "[1, 3200] loss: 0.312\n",
            "[1, 3300] loss: 0.387\n",
            "[1, 3400] loss: 0.320\n",
            "[1, 3500] loss: 0.278\n",
            "[1, 3600] loss: 0.308\n",
            "[1, 3700] loss: 0.253\n",
            "[1, 3800] loss: 0.334\n",
            "[1, 3900] loss: 0.351\n",
            "[1, 4000] loss: 0.375\n",
            "[1, 4100] loss: 0.305\n",
            "[1, 4200] loss: 0.302\n",
            "[1, 4300] loss: 0.295\n",
            "[1, 4400] loss: 0.293\n",
            "[1, 4500] loss: 0.270\n",
            "[1, 4600] loss: 0.316\n",
            "[1, 4700] loss: 0.250\n",
            "[1, 4800] loss: 0.280\n",
            "[1, 4900] loss: 0.368\n",
            "[1, 5000] loss: 0.245\n",
            "[1, 5100] loss: 0.339\n",
            "[1, 5200] loss: 0.266\n",
            "[1, 5300] loss: 0.272\n",
            "[1, 5400] loss: 0.241\n",
            "[1, 5500] loss: 0.312\n",
            "[1, 5600] loss: 0.268\n",
            "[1, 5700] loss: 0.317\n",
            "[1, 5800] loss: 0.306\n",
            "[1, 5900] loss: 0.347\n",
            "[1, 6000] loss: 0.312\n",
            "[2, 100] loss: 0.287\n",
            "[2, 200] loss: 0.251\n",
            "[2, 300] loss: 0.264\n",
            "[2, 400] loss: 0.283\n",
            "[2, 500] loss: 0.277\n",
            "[2, 600] loss: 0.270\n",
            "[2, 700] loss: 0.215\n",
            "[2, 800] loss: 0.285\n",
            "[2, 900] loss: 0.297\n",
            "[2, 1000] loss: 0.286\n",
            "[2, 1100] loss: 0.269\n",
            "[2, 1200] loss: 0.189\n",
            "[2, 1300] loss: 0.263\n",
            "[2, 1400] loss: 0.283\n",
            "[2, 1500] loss: 0.288\n",
            "[2, 1600] loss: 0.247\n",
            "[2, 1700] loss: 0.201\n",
            "[2, 1800] loss: 0.256\n",
            "[2, 1900] loss: 0.264\n",
            "[2, 2000] loss: 0.307\n",
            "[2, 2100] loss: 0.187\n",
            "[2, 2200] loss: 0.287\n",
            "[2, 2300] loss: 0.212\n",
            "[2, 2400] loss: 0.262\n",
            "[2, 2500] loss: 0.310\n",
            "[2, 2600] loss: 0.222\n",
            "[2, 2700] loss: 0.216\n",
            "[2, 2800] loss: 0.348\n",
            "[2, 2900] loss: 0.197\n",
            "[2, 3000] loss: 0.300\n",
            "[2, 3100] loss: 0.313\n",
            "[2, 3200] loss: 0.221\n",
            "[2, 3300] loss: 0.271\n",
            "[2, 3400] loss: 0.285\n",
            "[2, 3500] loss: 0.219\n",
            "[2, 3600] loss: 0.263\n",
            "[2, 3700] loss: 0.229\n",
            "[2, 3800] loss: 0.219\n",
            "[2, 3900] loss: 0.182\n",
            "[2, 4000] loss: 0.271\n",
            "[2, 4100] loss: 0.290\n",
            "[2, 4200] loss: 0.253\n",
            "[2, 4300] loss: 0.228\n",
            "[2, 4400] loss: 0.178\n",
            "[2, 4500] loss: 0.184\n",
            "[2, 4600] loss: 0.207\n",
            "[2, 4700] loss: 0.223\n",
            "[2, 4800] loss: 0.236\n",
            "[2, 4900] loss: 0.278\n",
            "[2, 5000] loss: 0.181\n",
            "[2, 5100] loss: 0.165\n",
            "[2, 5200] loss: 0.241\n",
            "[2, 5300] loss: 0.234\n",
            "[2, 5400] loss: 0.234\n",
            "[2, 5500] loss: 0.283\n",
            "[2, 5600] loss: 0.233\n",
            "[2, 5700] loss: 0.236\n",
            "[2, 5800] loss: 0.311\n",
            "[2, 5900] loss: 0.211\n",
            "[2, 6000] loss: 0.245\n",
            "[3, 100] loss: 0.158\n",
            "[3, 200] loss: 0.227\n",
            "[3, 300] loss: 0.155\n",
            "[3, 400] loss: 0.218\n",
            "[3, 500] loss: 0.176\n",
            "[3, 600] loss: 0.158\n",
            "[3, 700] loss: 0.296\n",
            "[3, 800] loss: 0.209\n",
            "[3, 900] loss: 0.257\n",
            "[3, 1000] loss: 0.254\n",
            "[3, 1100] loss: 0.191\n",
            "[3, 1200] loss: 0.174\n",
            "[3, 1300] loss: 0.167\n",
            "[3, 1400] loss: 0.196\n",
            "[3, 1500] loss: 0.204\n",
            "[3, 1600] loss: 0.177\n",
            "[3, 1700] loss: 0.243\n",
            "[3, 1800] loss: 0.131\n",
            "[3, 1900] loss: 0.243\n",
            "[3, 2000] loss: 0.293\n",
            "[3, 2100] loss: 0.190\n",
            "[3, 2200] loss: 0.221\n",
            "[3, 2300] loss: 0.211\n",
            "[3, 2400] loss: 0.201\n",
            "[3, 2500] loss: 0.170\n",
            "[3, 2600] loss: 0.202\n",
            "[3, 2700] loss: 0.203\n",
            "[3, 2800] loss: 0.241\n",
            "[3, 2900] loss: 0.228\n",
            "[3, 3000] loss: 0.199\n",
            "[3, 3100] loss: 0.237\n",
            "[3, 3200] loss: 0.186\n",
            "[3, 3300] loss: 0.226\n",
            "[3, 3400] loss: 0.213\n",
            "[3, 3500] loss: 0.240\n",
            "[3, 3600] loss: 0.187\n",
            "[3, 3700] loss: 0.229\n",
            "[3, 3800] loss: 0.234\n",
            "[3, 3900] loss: 0.194\n",
            "[3, 4000] loss: 0.218\n",
            "[3, 4100] loss: 0.204\n",
            "[3, 4200] loss: 0.198\n",
            "[3, 4300] loss: 0.214\n",
            "[3, 4400] loss: 0.213\n",
            "[3, 4500] loss: 0.237\n",
            "[3, 4600] loss: 0.251\n",
            "[3, 4700] loss: 0.180\n",
            "[3, 4800] loss: 0.166\n",
            "[3, 4900] loss: 0.185\n",
            "[3, 5000] loss: 0.231\n",
            "[3, 5100] loss: 0.244\n",
            "[3, 5200] loss: 0.212\n",
            "[3, 5300] loss: 0.165\n",
            "[3, 5400] loss: 0.217\n",
            "[3, 5500] loss: 0.148\n",
            "[3, 5600] loss: 0.168\n",
            "[3, 5700] loss: 0.229\n",
            "[3, 5800] loss: 0.241\n",
            "[3, 5900] loss: 0.181\n",
            "[3, 6000] loss: 0.267\n",
            "[4, 100] loss: 0.132\n",
            "[4, 200] loss: 0.203\n",
            "[4, 300] loss: 0.238\n",
            "[4, 400] loss: 0.207\n",
            "[4, 500] loss: 0.175\n",
            "[4, 600] loss: 0.196\n",
            "[4, 700] loss: 0.287\n",
            "[4, 800] loss: 0.265\n",
            "[4, 900] loss: 0.163\n",
            "[4, 1000] loss: 0.169\n",
            "[4, 1100] loss: 0.160\n",
            "[4, 1200] loss: 0.210\n",
            "[4, 1300] loss: 0.163\n",
            "[4, 1400] loss: 0.185\n",
            "[4, 1500] loss: 0.197\n",
            "[4, 1600] loss: 0.203\n",
            "[4, 1700] loss: 0.198\n",
            "[4, 1800] loss: 0.196\n",
            "[4, 1900] loss: 0.143\n",
            "[4, 2000] loss: 0.201\n",
            "[4, 2100] loss: 0.169\n",
            "[4, 2200] loss: 0.215\n",
            "[4, 2300] loss: 0.177\n",
            "[4, 2400] loss: 0.160\n",
            "[4, 2500] loss: 0.185\n",
            "[4, 2600] loss: 0.239\n",
            "[4, 2700] loss: 0.190\n",
            "[4, 2800] loss: 0.182\n",
            "[4, 2900] loss: 0.184\n",
            "[4, 3000] loss: 0.189\n",
            "[4, 3100] loss: 0.149\n",
            "[4, 3200] loss: 0.135\n",
            "[4, 3300] loss: 0.206\n",
            "[4, 3400] loss: 0.177\n",
            "[4, 3500] loss: 0.197\n",
            "[4, 3600] loss: 0.182\n",
            "[4, 3700] loss: 0.264\n",
            "[4, 3800] loss: 0.176\n",
            "[4, 3900] loss: 0.204\n",
            "[4, 4000] loss: 0.198\n",
            "[4, 4100] loss: 0.204\n",
            "[4, 4200] loss: 0.198\n",
            "[4, 4300] loss: 0.225\n",
            "[4, 4400] loss: 0.267\n",
            "[4, 4500] loss: 0.192\n",
            "[4, 4600] loss: 0.132\n",
            "[4, 4700] loss: 0.159\n",
            "[4, 4800] loss: 0.217\n",
            "[4, 4900] loss: 0.154\n",
            "[4, 5000] loss: 0.217\n",
            "[4, 5100] loss: 0.183\n",
            "[4, 5200] loss: 0.150\n",
            "[4, 5300] loss: 0.164\n",
            "[4, 5400] loss: 0.167\n",
            "[4, 5500] loss: 0.221\n",
            "[4, 5600] loss: 0.189\n",
            "[4, 5700] loss: 0.232\n",
            "[4, 5800] loss: 0.173\n",
            "[4, 5900] loss: 0.223\n",
            "[4, 6000] loss: 0.148\n",
            "[5, 100] loss: 0.173\n",
            "[5, 200] loss: 0.186\n",
            "[5, 300] loss: 0.173\n",
            "[5, 400] loss: 0.154\n",
            "[5, 500] loss: 0.140\n",
            "[5, 600] loss: 0.149\n",
            "[5, 700] loss: 0.178\n",
            "[5, 800] loss: 0.156\n",
            "[5, 900] loss: 0.159\n",
            "[5, 1000] loss: 0.140\n",
            "[5, 1100] loss: 0.172\n",
            "[5, 1200] loss: 0.146\n",
            "[5, 1300] loss: 0.174\n",
            "[5, 1400] loss: 0.202\n",
            "[5, 1500] loss: 0.175\n",
            "[5, 1600] loss: 0.195\n",
            "[5, 1700] loss: 0.163\n",
            "[5, 1800] loss: 0.143\n",
            "[5, 1900] loss: 0.209\n",
            "[5, 2000] loss: 0.151\n",
            "[5, 2100] loss: 0.174\n",
            "[5, 2200] loss: 0.202\n",
            "[5, 2300] loss: 0.151\n",
            "[5, 2400] loss: 0.119\n",
            "[5, 2500] loss: 0.195\n",
            "[5, 2600] loss: 0.152\n",
            "[5, 2700] loss: 0.119\n",
            "[5, 2800] loss: 0.180\n",
            "[5, 2900] loss: 0.159\n",
            "[5, 3000] loss: 0.125\n",
            "[5, 3100] loss: 0.179\n",
            "[5, 3200] loss: 0.218\n",
            "[5, 3300] loss: 0.132\n",
            "[5, 3400] loss: 0.201\n",
            "[5, 3500] loss: 0.174\n",
            "[5, 3600] loss: 0.122\n",
            "[5, 3700] loss: 0.116\n",
            "[5, 3800] loss: 0.169\n",
            "[5, 3900] loss: 0.208\n",
            "[5, 4000] loss: 0.174\n",
            "[5, 4100] loss: 0.197\n",
            "[5, 4200] loss: 0.197\n",
            "[5, 4300] loss: 0.168\n",
            "[5, 4400] loss: 0.216\n",
            "[5, 4500] loss: 0.213\n",
            "[5, 4600] loss: 0.187\n",
            "[5, 4700] loss: 0.196\n",
            "[5, 4800] loss: 0.192\n",
            "[5, 4900] loss: 0.183\n",
            "[5, 5000] loss: 0.181\n",
            "[5, 5100] loss: 0.139\n",
            "[5, 5200] loss: 0.209\n",
            "[5, 5300] loss: 0.204\n",
            "[5, 5400] loss: 0.204\n",
            "[5, 5500] loss: 0.213\n",
            "[5, 5600] loss: 0.178\n",
            "[5, 5700] loss: 0.183\n",
            "[5, 5800] loss: 0.257\n",
            "[5, 5900] loss: 0.158\n",
            "[5, 6000] loss: 0.169\n",
            "Accuracy: 94.18%\n",
            "Model 2\n",
            "[1, 100] loss: 1.984\n",
            "[1, 200] loss: 1.058\n",
            "[1, 300] loss: 0.741\n",
            "[1, 400] loss: 0.569\n",
            "[1, 500] loss: 0.508\n",
            "[1, 600] loss: 0.463\n",
            "[1, 700] loss: 0.451\n",
            "[1, 800] loss: 0.469\n",
            "[1, 900] loss: 0.398\n",
            "[1, 1000] loss: 0.404\n",
            "[1, 1100] loss: 0.429\n",
            "[1, 1200] loss: 0.388\n",
            "[1, 1300] loss: 0.409\n",
            "[1, 1400] loss: 0.410\n",
            "[1, 1500] loss: 0.378\n",
            "[1, 1600] loss: 0.417\n",
            "[1, 1700] loss: 0.342\n",
            "[1, 1800] loss: 0.320\n",
            "[1, 1900] loss: 0.361\n",
            "[1, 2000] loss: 0.308\n",
            "[1, 2100] loss: 0.300\n",
            "[1, 2200] loss: 0.331\n",
            "[1, 2300] loss: 0.336\n",
            "[1, 2400] loss: 0.299\n",
            "[1, 2500] loss: 0.319\n",
            "[1, 2600] loss: 0.322\n",
            "[1, 2700] loss: 0.298\n",
            "[1, 2800] loss: 0.279\n",
            "[1, 2900] loss: 0.284\n",
            "[1, 3000] loss: 0.256\n",
            "[1, 3100] loss: 0.246\n",
            "[1, 3200] loss: 0.285\n",
            "[1, 3300] loss: 0.324\n",
            "[1, 3400] loss: 0.248\n",
            "[1, 3500] loss: 0.270\n",
            "[1, 3600] loss: 0.256\n",
            "[1, 3700] loss: 0.252\n",
            "[1, 3800] loss: 0.228\n",
            "[1, 3900] loss: 0.240\n",
            "[1, 4000] loss: 0.227\n",
            "[1, 4100] loss: 0.255\n",
            "[1, 4200] loss: 0.234\n",
            "[1, 4300] loss: 0.262\n",
            "[1, 4400] loss: 0.239\n",
            "[1, 4500] loss: 0.236\n",
            "[1, 4600] loss: 0.246\n",
            "[1, 4700] loss: 0.237\n",
            "[1, 4800] loss: 0.229\n",
            "[1, 4900] loss: 0.271\n",
            "[1, 5000] loss: 0.265\n",
            "[1, 5100] loss: 0.246\n",
            "[1, 5200] loss: 0.241\n",
            "[1, 5300] loss: 0.223\n",
            "[1, 5400] loss: 0.239\n",
            "[1, 5500] loss: 0.225\n",
            "[1, 5600] loss: 0.226\n",
            "[1, 5700] loss: 0.227\n",
            "[1, 5800] loss: 0.232\n",
            "[1, 5900] loss: 0.239\n",
            "[1, 6000] loss: 0.216\n",
            "[2, 100] loss: 0.212\n",
            "[2, 200] loss: 0.221\n",
            "[2, 300] loss: 0.222\n",
            "[2, 400] loss: 0.215\n",
            "[2, 500] loss: 0.194\n",
            "[2, 600] loss: 0.192\n",
            "[2, 700] loss: 0.165\n",
            "[2, 800] loss: 0.203\n",
            "[2, 900] loss: 0.210\n",
            "[2, 1000] loss: 0.211\n",
            "[2, 1100] loss: 0.138\n",
            "[2, 1200] loss: 0.193\n",
            "[2, 1300] loss: 0.172\n",
            "[2, 1400] loss: 0.185\n",
            "[2, 1500] loss: 0.160\n",
            "[2, 1600] loss: 0.177\n",
            "[2, 1700] loss: 0.177\n",
            "[2, 1800] loss: 0.179\n",
            "[2, 1900] loss: 0.172\n",
            "[2, 2000] loss: 0.195\n",
            "[2, 2100] loss: 0.165\n",
            "[2, 2200] loss: 0.188\n",
            "[2, 2300] loss: 0.161\n",
            "[2, 2400] loss: 0.160\n",
            "[2, 2500] loss: 0.162\n",
            "[2, 2600] loss: 0.163\n",
            "[2, 2700] loss: 0.174\n",
            "[2, 2800] loss: 0.152\n",
            "[2, 2900] loss: 0.192\n",
            "[2, 3000] loss: 0.186\n",
            "[2, 3100] loss: 0.161\n",
            "[2, 3200] loss: 0.155\n",
            "[2, 3300] loss: 0.181\n",
            "[2, 3400] loss: 0.179\n",
            "[2, 3500] loss: 0.210\n",
            "[2, 3600] loss: 0.146\n",
            "[2, 3700] loss: 0.161\n",
            "[2, 3800] loss: 0.167\n",
            "[2, 3900] loss: 0.151\n",
            "[2, 4000] loss: 0.156\n",
            "[2, 4100] loss: 0.207\n",
            "[2, 4200] loss: 0.182\n",
            "[2, 4300] loss: 0.129\n",
            "[2, 4400] loss: 0.170\n",
            "[2, 4500] loss: 0.208\n",
            "[2, 4600] loss: 0.176\n",
            "[2, 4700] loss: 0.185\n",
            "[2, 4800] loss: 0.157\n",
            "[2, 4900] loss: 0.159\n",
            "[2, 5000] loss: 0.138\n",
            "[2, 5100] loss: 0.165\n",
            "[2, 5200] loss: 0.133\n",
            "[2, 5300] loss: 0.171\n",
            "[2, 5400] loss: 0.149\n",
            "[2, 5500] loss: 0.175\n",
            "[2, 5600] loss: 0.129\n",
            "[2, 5700] loss: 0.155\n",
            "[2, 5800] loss: 0.135\n",
            "[2, 5900] loss: 0.155\n",
            "[2, 6000] loss: 0.157\n",
            "[3, 100] loss: 0.143\n",
            "[3, 200] loss: 0.164\n",
            "[3, 300] loss: 0.102\n",
            "[3, 400] loss: 0.134\n",
            "[3, 500] loss: 0.105\n",
            "[3, 600] loss: 0.173\n",
            "[3, 700] loss: 0.120\n",
            "[3, 800] loss: 0.129\n",
            "[3, 900] loss: 0.148\n",
            "[3, 1000] loss: 0.130\n",
            "[3, 1100] loss: 0.129\n",
            "[3, 1200] loss: 0.120\n",
            "[3, 1300] loss: 0.164\n",
            "[3, 1400] loss: 0.131\n",
            "[3, 1500] loss: 0.151\n",
            "[3, 1600] loss: 0.128\n",
            "[3, 1700] loss: 0.110\n",
            "[3, 1800] loss: 0.128\n",
            "[3, 1900] loss: 0.164\n",
            "[3, 2000] loss: 0.130\n",
            "[3, 2100] loss: 0.126\n",
            "[3, 2200] loss: 0.124\n",
            "[3, 2300] loss: 0.108\n",
            "[3, 2400] loss: 0.109\n",
            "[3, 2500] loss: 0.145\n",
            "[3, 2600] loss: 0.159\n",
            "[3, 2700] loss: 0.093\n",
            "[3, 2800] loss: 0.112\n",
            "[3, 2900] loss: 0.159\n",
            "[3, 3000] loss: 0.157\n",
            "[3, 3100] loss: 0.132\n",
            "[3, 3200] loss: 0.108\n",
            "[3, 3300] loss: 0.127\n",
            "[3, 3400] loss: 0.131\n",
            "[3, 3500] loss: 0.135\n",
            "[3, 3600] loss: 0.098\n",
            "[3, 3700] loss: 0.116\n",
            "[3, 3800] loss: 0.143\n",
            "[3, 3900] loss: 0.105\n",
            "[3, 4000] loss: 0.101\n",
            "[3, 4100] loss: 0.155\n",
            "[3, 4200] loss: 0.137\n",
            "[3, 4300] loss: 0.129\n",
            "[3, 4400] loss: 0.114\n",
            "[3, 4500] loss: 0.146\n",
            "[3, 4600] loss: 0.166\n",
            "[3, 4700] loss: 0.103\n",
            "[3, 4800] loss: 0.116\n",
            "[3, 4900] loss: 0.113\n",
            "[3, 5000] loss: 0.118\n",
            "[3, 5100] loss: 0.121\n",
            "[3, 5200] loss: 0.182\n",
            "[3, 5300] loss: 0.123\n",
            "[3, 5400] loss: 0.121\n",
            "[3, 5500] loss: 0.116\n",
            "[3, 5600] loss: 0.095\n",
            "[3, 5700] loss: 0.122\n",
            "[3, 5800] loss: 0.127\n",
            "[3, 5900] loss: 0.106\n",
            "[3, 6000] loss: 0.108\n",
            "[4, 100] loss: 0.104\n",
            "[4, 200] loss: 0.117\n",
            "[4, 300] loss: 0.109\n",
            "[4, 400] loss: 0.099\n",
            "[4, 500] loss: 0.087\n",
            "[4, 600] loss: 0.094\n",
            "[4, 700] loss: 0.097\n",
            "[4, 800] loss: 0.116\n",
            "[4, 900] loss: 0.103\n",
            "[4, 1000] loss: 0.102\n",
            "[4, 1100] loss: 0.103\n",
            "[4, 1200] loss: 0.096\n",
            "[4, 1300] loss: 0.080\n",
            "[4, 1400] loss: 0.107\n",
            "[4, 1500] loss: 0.098\n",
            "[4, 1600] loss: 0.116\n",
            "[4, 1700] loss: 0.095\n",
            "[4, 1800] loss: 0.113\n",
            "[4, 1900] loss: 0.119\n",
            "[4, 2000] loss: 0.105\n",
            "[4, 2100] loss: 0.114\n",
            "[4, 2200] loss: 0.125\n",
            "[4, 2300] loss: 0.099\n",
            "[4, 2400] loss: 0.106\n",
            "[4, 2500] loss: 0.091\n",
            "[4, 2600] loss: 0.106\n",
            "[4, 2700] loss: 0.104\n",
            "[4, 2800] loss: 0.089\n",
            "[4, 2900] loss: 0.143\n",
            "[4, 3000] loss: 0.097\n",
            "[4, 3100] loss: 0.102\n",
            "[4, 3200] loss: 0.111\n",
            "[4, 3300] loss: 0.085\n",
            "[4, 3400] loss: 0.078\n",
            "[4, 3500] loss: 0.120\n",
            "[4, 3600] loss: 0.120\n",
            "[4, 3700] loss: 0.087\n",
            "[4, 3800] loss: 0.098\n",
            "[4, 3900] loss: 0.119\n",
            "[4, 4000] loss: 0.141\n",
            "[4, 4100] loss: 0.115\n",
            "[4, 4200] loss: 0.098\n",
            "[4, 4300] loss: 0.090\n",
            "[4, 4400] loss: 0.117\n",
            "[4, 4500] loss: 0.102\n",
            "[4, 4600] loss: 0.124\n",
            "[4, 4700] loss: 0.096\n",
            "[4, 4800] loss: 0.087\n",
            "[4, 4900] loss: 0.120\n",
            "[4, 5000] loss: 0.090\n",
            "[4, 5100] loss: 0.097\n",
            "[4, 5200] loss: 0.117\n",
            "[4, 5300] loss: 0.101\n",
            "[4, 5400] loss: 0.064\n",
            "[4, 5500] loss: 0.087\n",
            "[4, 5600] loss: 0.111\n",
            "[4, 5700] loss: 0.112\n",
            "[4, 5800] loss: 0.079\n",
            "[4, 5900] loss: 0.085\n",
            "[4, 6000] loss: 0.090\n",
            "[5, 100] loss: 0.086\n",
            "[5, 200] loss: 0.088\n",
            "[5, 300] loss: 0.074\n",
            "[5, 400] loss: 0.075\n",
            "[5, 500] loss: 0.070\n",
            "[5, 600] loss: 0.073\n",
            "[5, 700] loss: 0.080\n",
            "[5, 800] loss: 0.096\n",
            "[5, 900] loss: 0.068\n",
            "[5, 1000] loss: 0.085\n",
            "[5, 1100] loss: 0.088\n",
            "[5, 1200] loss: 0.087\n",
            "[5, 1300] loss: 0.078\n",
            "[5, 1400] loss: 0.102\n",
            "[5, 1500] loss: 0.106\n",
            "[5, 1600] loss: 0.109\n",
            "[5, 1700] loss: 0.090\n",
            "[5, 1800] loss: 0.095\n",
            "[5, 1900] loss: 0.072\n",
            "[5, 2000] loss: 0.081\n",
            "[5, 2100] loss: 0.090\n",
            "[5, 2200] loss: 0.058\n",
            "[5, 2300] loss: 0.071\n",
            "[5, 2400] loss: 0.084\n",
            "[5, 2500] loss: 0.084\n",
            "[5, 2600] loss: 0.085\n",
            "[5, 2700] loss: 0.095\n",
            "[5, 2800] loss: 0.082\n",
            "[5, 2900] loss: 0.099\n",
            "[5, 3000] loss: 0.085\n",
            "[5, 3100] loss: 0.078\n",
            "[5, 3200] loss: 0.090\n",
            "[5, 3300] loss: 0.087\n",
            "[5, 3400] loss: 0.079\n",
            "[5, 3500] loss: 0.119\n",
            "[5, 3600] loss: 0.098\n",
            "[5, 3700] loss: 0.083\n",
            "[5, 3800] loss: 0.102\n",
            "[5, 3900] loss: 0.077\n",
            "[5, 4000] loss: 0.067\n",
            "[5, 4100] loss: 0.077\n",
            "[5, 4200] loss: 0.060\n",
            "[5, 4300] loss: 0.071\n",
            "[5, 4400] loss: 0.066\n",
            "[5, 4500] loss: 0.091\n",
            "[5, 4600] loss: 0.086\n",
            "[5, 4700] loss: 0.089\n",
            "[5, 4800] loss: 0.082\n",
            "[5, 4900] loss: 0.098\n",
            "[5, 5000] loss: 0.110\n",
            "[5, 5100] loss: 0.086\n",
            "[5, 5200] loss: 0.074\n",
            "[5, 5300] loss: 0.103\n",
            "[5, 5400] loss: 0.103\n",
            "[5, 5500] loss: 0.080\n",
            "[5, 5600] loss: 0.098\n",
            "[5, 5700] loss: 0.088\n",
            "[5, 5800] loss: 0.074\n",
            "[5, 5900] loss: 0.103\n",
            "[5, 6000] loss: 0.076\n",
            "Accuracy: 96.95%\n",
            "Model 3\n",
            "[1, 100] loss: 2.303\n",
            "[1, 200] loss: 2.300\n",
            "[1, 300] loss: 2.261\n",
            "[1, 400] loss: 2.156\n",
            "[1, 500] loss: 1.996\n",
            "[1, 600] loss: 1.896\n",
            "[1, 700] loss: 1.796\n",
            "[1, 800] loss: 1.785\n",
            "[1, 900] loss: 1.711\n",
            "[1, 1000] loss: 1.693\n",
            "[1, 1100] loss: 1.690\n",
            "[1, 1200] loss: 1.675\n",
            "[1, 1300] loss: 1.646\n",
            "[1, 1400] loss: 1.716\n",
            "[1, 1500] loss: 1.639\n",
            "[1, 1600] loss: 1.617\n",
            "[1, 1700] loss: 1.624\n",
            "[1, 1800] loss: 1.661\n",
            "[1, 1900] loss: 1.623\n",
            "[1, 2000] loss: 1.571\n",
            "[1, 2100] loss: 1.591\n",
            "[1, 2200] loss: 1.565\n",
            "[1, 2300] loss: 1.589\n",
            "[1, 2400] loss: 1.557\n",
            "[1, 2500] loss: 1.579\n",
            "[1, 2600] loss: 1.536\n",
            "[1, 2700] loss: 1.512\n",
            "[1, 2800] loss: 1.553\n",
            "[1, 2900] loss: 1.487\n",
            "[1, 3000] loss: 1.472\n",
            "[1, 3100] loss: 1.424\n",
            "[1, 3200] loss: 1.430\n",
            "[1, 3300] loss: 1.389\n",
            "[1, 3400] loss: 1.349\n",
            "[1, 3500] loss: 1.399\n",
            "[1, 3600] loss: 1.385\n",
            "[1, 3700] loss: 1.378\n",
            "[1, 3800] loss: 1.307\n",
            "[1, 3900] loss: 1.324\n",
            "[1, 4000] loss: 1.325\n",
            "[1, 4100] loss: 1.341\n",
            "[1, 4200] loss: 1.319\n",
            "[1, 4300] loss: 1.232\n",
            "[1, 4400] loss: 1.276\n",
            "[1, 4500] loss: 1.295\n",
            "[1, 4600] loss: 1.205\n",
            "[1, 4700] loss: 1.151\n",
            "[1, 4800] loss: 1.149\n",
            "[1, 4900] loss: 1.232\n",
            "[1, 5000] loss: 1.227\n",
            "[1, 5100] loss: 1.167\n",
            "[1, 5200] loss: 1.133\n",
            "[1, 5300] loss: 1.051\n",
            "[1, 5400] loss: 1.072\n",
            "[1, 5500] loss: 1.091\n",
            "[1, 5600] loss: 1.156\n",
            "[1, 5700] loss: 1.121\n",
            "[1, 5800] loss: 1.085\n",
            "[1, 5900] loss: 1.114\n",
            "[1, 6000] loss: 1.161\n",
            "[2, 100] loss: 1.134\n",
            "[2, 200] loss: 1.050\n",
            "[2, 300] loss: 1.052\n",
            "[2, 400] loss: 1.053\n",
            "[2, 500] loss: 1.064\n",
            "[2, 600] loss: 1.100\n",
            "[2, 700] loss: 0.995\n",
            "[2, 800] loss: 1.036\n",
            "[2, 900] loss: 0.979\n",
            "[2, 1000] loss: 1.035\n",
            "[2, 1100] loss: 1.056\n",
            "[2, 1200] loss: 1.126\n",
            "[2, 1300] loss: 1.101\n",
            "[2, 1400] loss: 1.040\n",
            "[2, 1500] loss: 1.091\n",
            "[2, 1600] loss: 1.025\n",
            "[2, 1700] loss: 1.078\n",
            "[2, 1800] loss: 1.046\n",
            "[2, 1900] loss: 1.050\n",
            "[2, 2000] loss: 1.051\n",
            "[2, 2100] loss: 1.068\n",
            "[2, 2200] loss: 1.002\n",
            "[2, 2300] loss: 1.081\n",
            "[2, 2400] loss: 1.027\n",
            "[2, 2500] loss: 1.080\n",
            "[2, 2600] loss: 1.065\n",
            "[2, 2700] loss: 1.063\n",
            "[2, 2800] loss: 1.094\n",
            "[2, 2900] loss: 1.008\n",
            "[2, 3000] loss: 1.098\n",
            "[2, 3100] loss: 1.047\n",
            "[2, 3200] loss: 1.070\n",
            "[2, 3300] loss: 1.045\n",
            "[2, 3400] loss: 1.073\n",
            "[2, 3500] loss: 0.971\n",
            "[2, 3600] loss: 1.036\n",
            "[2, 3700] loss: 1.067\n",
            "[2, 3800] loss: 1.006\n",
            "[2, 3900] loss: 1.074\n",
            "[2, 4000] loss: 1.015\n",
            "[2, 4100] loss: 1.029\n",
            "[2, 4200] loss: 1.005\n",
            "[2, 4300] loss: 0.977\n",
            "[2, 4400] loss: 1.077\n",
            "[2, 4500] loss: 1.118\n",
            "[2, 4600] loss: 1.009\n",
            "[2, 4700] loss: 0.995\n",
            "[2, 4800] loss: 1.004\n",
            "[2, 4900] loss: 1.048\n",
            "[2, 5000] loss: 1.084\n",
            "[2, 5100] loss: 1.005\n",
            "[2, 5200] loss: 1.047\n",
            "[2, 5300] loss: 0.994\n",
            "[2, 5400] loss: 0.904\n",
            "[2, 5500] loss: 1.012\n",
            "[2, 5600] loss: 0.958\n",
            "[2, 5700] loss: 0.862\n",
            "[2, 5800] loss: 1.022\n",
            "[2, 5900] loss: 0.895\n",
            "[2, 6000] loss: 1.002\n",
            "[3, 100] loss: 0.932\n",
            "[3, 200] loss: 0.970\n",
            "[3, 300] loss: 0.929\n",
            "[3, 400] loss: 0.958\n",
            "[3, 500] loss: 0.911\n",
            "[3, 600] loss: 0.859\n",
            "[3, 700] loss: 0.829\n",
            "[3, 800] loss: 0.932\n",
            "[3, 900] loss: 0.934\n",
            "[3, 1000] loss: 0.964\n",
            "[3, 1100] loss: 0.978\n",
            "[3, 1200] loss: 0.869\n",
            "[3, 1300] loss: 0.965\n",
            "[3, 1400] loss: 0.910\n",
            "[3, 1500] loss: 0.910\n",
            "[3, 1600] loss: 0.915\n",
            "[3, 1700] loss: 0.988\n",
            "[3, 1800] loss: 0.932\n",
            "[3, 1900] loss: 0.935\n",
            "[3, 2000] loss: 0.894\n",
            "[3, 2100] loss: 0.871\n",
            "[3, 2200] loss: 0.881\n",
            "[3, 2300] loss: 0.842\n",
            "[3, 2400] loss: 0.824\n",
            "[3, 2500] loss: 0.884\n",
            "[3, 2600] loss: 0.887\n",
            "[3, 2700] loss: 0.906\n",
            "[3, 2800] loss: 0.850\n",
            "[3, 2900] loss: 0.895\n",
            "[3, 3000] loss: 0.861\n",
            "[3, 3100] loss: 0.882\n",
            "[3, 3200] loss: 0.887\n",
            "[3, 3300] loss: 0.835\n",
            "[3, 3400] loss: 0.862\n",
            "[3, 3500] loss: 0.910\n",
            "[3, 3600] loss: 0.855\n",
            "[3, 3700] loss: 0.911\n",
            "[3, 3800] loss: 0.850\n",
            "[3, 3900] loss: 0.885\n",
            "[3, 4000] loss: 0.870\n",
            "[3, 4100] loss: 0.966\n",
            "[3, 4200] loss: 0.876\n",
            "[3, 4300] loss: 0.866\n",
            "[3, 4400] loss: 0.897\n",
            "[3, 4500] loss: 0.833\n",
            "[3, 4600] loss: 0.859\n",
            "[3, 4700] loss: 0.915\n",
            "[3, 4800] loss: 0.886\n",
            "[3, 4900] loss: 0.869\n",
            "[3, 5000] loss: 0.890\n",
            "[3, 5100] loss: 0.830\n",
            "[3, 5200] loss: 0.868\n",
            "[3, 5300] loss: 0.837\n",
            "[3, 5400] loss: 0.895\n",
            "[3, 5500] loss: 0.860\n",
            "[3, 5600] loss: 0.860\n",
            "[3, 5700] loss: 0.825\n",
            "[3, 5800] loss: 0.868\n",
            "[3, 5900] loss: 0.937\n",
            "[3, 6000] loss: 0.872\n",
            "[4, 100] loss: 0.848\n",
            "[4, 200] loss: 0.904\n",
            "[4, 300] loss: 0.885\n",
            "[4, 400] loss: 0.847\n",
            "[4, 500] loss: 0.820\n",
            "[4, 600] loss: 0.839\n",
            "[4, 700] loss: 0.761\n",
            "[4, 800] loss: 0.777\n",
            "[4, 900] loss: 0.826\n",
            "[4, 1000] loss: 0.767\n",
            "[4, 1100] loss: 0.823\n",
            "[4, 1200] loss: 0.745\n",
            "[4, 1300] loss: 0.735\n",
            "[4, 1400] loss: 0.676\n",
            "[4, 1500] loss: 0.778\n",
            "[4, 1600] loss: 0.802\n",
            "[4, 1700] loss: 0.797\n",
            "[4, 1800] loss: 0.821\n",
            "[4, 1900] loss: 0.807\n",
            "[4, 2000] loss: 0.776\n",
            "[4, 2100] loss: 0.723\n",
            "[4, 2200] loss: 0.735\n",
            "[4, 2300] loss: 0.716\n",
            "[4, 2400] loss: 0.759\n",
            "[4, 2500] loss: 0.802\n",
            "[4, 2600] loss: 0.690\n",
            "[4, 2700] loss: 0.708\n",
            "[4, 2800] loss: 0.769\n",
            "[4, 2900] loss: 0.784\n",
            "[4, 3000] loss: 0.811\n",
            "[4, 3100] loss: 0.720\n",
            "[4, 3200] loss: 0.762\n",
            "[4, 3300] loss: 0.743\n",
            "[4, 3400] loss: 0.749\n",
            "[4, 3500] loss: 0.758\n",
            "[4, 3600] loss: 0.749\n",
            "[4, 3700] loss: 0.769\n",
            "[4, 3800] loss: 0.792\n",
            "[4, 3900] loss: 0.803\n",
            "[4, 4000] loss: 0.723\n",
            "[4, 4100] loss: 0.717\n",
            "[4, 4200] loss: 0.728\n",
            "[4, 4300] loss: 0.730\n",
            "[4, 4400] loss: 0.717\n",
            "[4, 4500] loss: 0.797\n",
            "[4, 4600] loss: 0.744\n",
            "[4, 4700] loss: 0.742\n",
            "[4, 4800] loss: 0.696\n",
            "[4, 4900] loss: 0.804\n",
            "[4, 5000] loss: 0.811\n",
            "[4, 5100] loss: 0.773\n",
            "[4, 5200] loss: 0.787\n",
            "[4, 5300] loss: 0.756\n",
            "[4, 5400] loss: 0.763\n",
            "[4, 5500] loss: 0.783\n",
            "[4, 5600] loss: 0.673\n",
            "[4, 5700] loss: 0.796\n",
            "[4, 5800] loss: 0.708\n",
            "[4, 5900] loss: 0.755\n",
            "[4, 6000] loss: 0.792\n",
            "[5, 100] loss: 0.664\n",
            "[5, 200] loss: 0.702\n",
            "[5, 300] loss: 0.718\n",
            "[5, 400] loss: 0.733\n",
            "[5, 500] loss: 0.744\n",
            "[5, 600] loss: 0.723\n",
            "[5, 700] loss: 0.766\n",
            "[5, 800] loss: 0.717\n",
            "[5, 900] loss: 0.690\n",
            "[5, 1000] loss: 0.692\n",
            "[5, 1100] loss: 0.801\n",
            "[5, 1200] loss: 0.709\n",
            "[5, 1300] loss: 0.790\n",
            "[5, 1400] loss: 0.758\n",
            "[5, 1500] loss: 0.724\n",
            "[5, 1600] loss: 0.733\n",
            "[5, 1700] loss: 0.764\n",
            "[5, 1800] loss: 0.740\n",
            "[5, 1900] loss: 0.756\n",
            "[5, 2000] loss: 0.774\n",
            "[5, 2100] loss: 0.715\n",
            "[5, 2200] loss: 0.664\n",
            "[5, 2300] loss: 0.666\n",
            "[5, 2400] loss: 0.733\n",
            "[5, 2500] loss: 0.783\n",
            "[5, 2600] loss: 0.775\n",
            "[5, 2700] loss: 0.735\n",
            "[5, 2800] loss: 0.617\n",
            "[5, 2900] loss: 0.722\n",
            "[5, 3000] loss: 0.671\n",
            "[5, 3100] loss: 0.729\n",
            "[5, 3200] loss: 0.718\n",
            "[5, 3300] loss: 0.735\n",
            "[5, 3400] loss: 0.778\n",
            "[5, 3500] loss: 0.800\n",
            "[5, 3600] loss: 0.666\n",
            "[5, 3700] loss: 0.731\n",
            "[5, 3800] loss: 0.680\n",
            "[5, 3900] loss: 0.710\n",
            "[5, 4000] loss: 0.711\n",
            "[5, 4100] loss: 0.748\n",
            "[5, 4200] loss: 0.768\n",
            "[5, 4300] loss: 0.769\n",
            "[5, 4400] loss: 0.718\n",
            "[5, 4500] loss: 0.676\n",
            "[5, 4600] loss: 0.753\n",
            "[5, 4700] loss: 0.703\n",
            "[5, 4800] loss: 0.739\n",
            "[5, 4900] loss: 0.684\n",
            "[5, 5000] loss: 0.693\n",
            "[5, 5100] loss: 0.783\n",
            "[5, 5200] loss: 0.767\n",
            "[5, 5300] loss: 0.752\n",
            "[5, 5400] loss: 0.779\n",
            "[5, 5500] loss: 0.712\n",
            "[5, 5600] loss: 0.692\n",
            "[5, 5700] loss: 0.716\n",
            "[5, 5800] loss: 0.696\n",
            "[5, 5900] loss: 0.719\n",
            "[5, 6000] loss: 0.715\n",
            "Accuracy: 72.21%\n",
            "Accuracies: [94.18, 96.95, 72.21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracies:\", accuracies)"
      ],
      "metadata": {
        "id": "OJN_oK30kqKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4efb21c-edcf-4624-976a-318e42dfd6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [94.18, 96.95, 72.21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berikut akurasi untuk setiap model dengan konfigurasi yang berbeda:\n",
        "\n",
        "\n",
        "*   Model 1 :  Fungsi aktivasi ReLU = 94.18\n",
        "*   Model 2 : Model 2: Fungsi aktivasi Sigmoid = 96.95\n",
        "*   Model 3: Fungsi aktivasi Softmax = 72.21\n",
        "\n",
        "Berdasarkan hasil evaluasi, terlihat bahwa model yang menggunakan aktivasi sigmoid memberikan performa terbaik dengan akurasi 96.95%. Model yang menggunakan aktivasi ReLU juga memberikan performa cukup baik dengan akurasi 94.18%. Sedangkan model yang menggunakan aktivasi Softmax memberirakan performa paling buruk dengan akurasi 72.21%. Dengan demikian pilihan fungsi aktivasi sangat berpengaruh dalam memilih fungsi aktivasi yang tepat sangat penting untuk mendapatkan hasil yang baik dalam tugas klasifikasi.\n",
        "\n"
      ],
      "metadata": {
        "id": "nV-R3-nvkbFg"
      }
    }
  ]
}